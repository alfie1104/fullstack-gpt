from langchain.document_loaders import SitemapLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings
import streamlit as st

import asyncio
asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) # windowì—ì„œ NotImplementedErrorê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ì¶”ê°€í•˜ì˜€ìŒ

#  [SitemapLoader]
#  - sitemap.xml íŒŒì¼ì„ ê°€ì§€ê³  ìˆëŠ” ì‚¬ì´íŠ¸ì— ëŒ€í•´ì„œëŠ” SitemapLoaderë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.
#  - AsyncChromiumLoaderì˜ ê²½ìš° ì—¬ëŸ¬ê°œì˜ ì‚¬ì´íŠ¸ì— ëŒ€í•´ì„œ html ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ë ¤ê³  í•  ê²½ìš°, chromium browserê°€ ì—¬ëŸ¬ê°œ ì—´ë ¤ì•¼ í•˜ë¯€ë¡œ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆì§€ë§Œ
#  - SitemapLoaderëŠ” chromiumì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì„œ ìœ„ì™€ ê°™ì€ ë¬¸ì œê°€ ì—†ìŒ
#  - ë˜í•œ AsyncChromiumLoaderì™€ ë‹¬ë¦¬ SitemapLoaderëŠ” html ë°ì´í„°ë¥¼ ì •ë¦¬í•´ì„œ textë§Œ ë½‘ì•„ì˜´(ê·¸ëŸ°ë° ë©”ë‰´, ë„¤ë¹„ê²Œì´ì…˜ ë“±ì˜ í…ìŠ¤íŠ¸ë“¤ë„ ë½‘ì•„ì˜¤ê¸° ë•Œë¬¸ì— í•„ìš”ì—†ëŠ” ë‚´ìš©ì´ ë§ìŒ)
#  - ë„ˆë¬´ ë¹ ë¥´ê²Œ scrappingì„ í•˜ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ default ì„¤ì •ìœ¼ë¡œ SitemapLoaderëŠ” 1ì´ˆì— í•œë²ˆì”© ìš”ì²­ì„ ë³´ëƒ„
#  - ì†ë„ë¥´ ë” ì¤„ì´ë ¤ë©´ SitemapLoader(url).requests_per_second í•¨ìˆ˜ë¥¼ í†µí•´ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ
#  - SitemapLoaderëŠ” pageë¡œë¶€í„° ëª¨ë“  textë¥¼ ì¶”ì¶œí•˜ê³  htmlì„ ì œê±°í•˜ê¸° ìœ„í•´ ë‚´ë¶€ì ìœ¼ë¡œ beautiful soup(soup is just bunch of html) ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŒ.
#  - parsing_function ì†ì„±ì„ ì„¤ì •í•˜ë©´ beautiful soupì˜ ë™ì‘ì„ ì¡°ì •í•  ìˆ˜ ìˆìŒ

def parse_page(soup):
    # beautiful soupê°€ ìƒì„±í•˜ëŠ” soup objectë¥¼ ì´ìš©í•´ì„œ í•„ìš”í•œ ë°ì´í„°ë§Œ ì¶”ì¶œ

    # ë‹¤ìŒê³¼ ê°™ì´ headerì™€ footer íƒœê·¸ë¥¼ ì°¾ì•„ì„œ soupì—ì„œ ì œê±°í•  ìˆ˜ ìˆìŒ
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        header.decompose()
    if footer:
        footer.decompose()
    # return soup.get_text() # headerì™€ footerë¥¼ ì œê±°í•œ ë‚˜ë¨¸ì§€ textë¥¼ ë°˜í™˜
    
    # soupì—ì„œ ë°˜í™˜í•œ textì—ì„œ í•„ìš”ì—†ëŠ” ë¬¸ìë“¤ì„ ì œê±°
    return (
        str(soup.get_text())
        .replace("\n"," ")
        .replace("\xa0"," ") # \xa0 : non-breaking-space
        .replace("CloseSearch Submit Blog", "")
    )


@st.cache_data(show_spinner="Loading website...")
def load_website(url):    
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200
    )

    loader = SitemapLoader(
        url 
        # , filter_urls=["https://openai.com/blog/data-partnerships"] # filter_urlsë¥¼ ì´ìš©í•˜ë©´ sitemap.xmlì— ìˆëŠ” ì‚¬ì´íŠ¸ ë“¤ ì¤‘ íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
        # , filter_urls=[r"^(?!.*\/blog\/).*"] # ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ì„œ /blog/ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ë§Œ ê°€ì ¸ì˜´
        # , filter_urls=[r"^(.*\/blog\/).*"] # ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ì„œ /blog/ë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ì´íŠ¸ë§Œ ê°€ì ¸ì˜´
        , parsing_function=parse_page # SitemapLoaderê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” beautiful soupì˜ ë™ì‘ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ì„œ parsing_function ì†ì„±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
    )
    loader.requests_per_second = 5
    # docs = loader.load()
    docs = loader.load_and_split(text_splitter=splitter) # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ê²Œ ì˜ë¼ì„œ ë°˜í™˜í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ text_splittertì‚¬ìš©
    # return docs

    vector_store = FAISS.from_documents(docs, OpenAIEmbeddings()) # documentë¥¼ ê²€ìƒ‰ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„ë² ë”© í›„ vector storeì— ì €ì¥
    return vector_store.as_retriever() # vector storeë¥¼ chainì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ retriever(ì§ˆë¬¸ ì¿¼ë¦¬ì— ì í•©í•œ documentë¥¼ ë°˜í™˜í•´ì¤Œ)ë¡œ ë°˜í™˜(retrieverëŠ” invoke ë©”ì†Œë“œë¥¼ ê°€ì§€ê³  ìˆì–´ì„œ chainì—ì„œ ì‹¤í–‰ê°€ëŠ¥í•¨)
    

st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ’»"
)

st.title("SiteGPT")


st.markdown("""
Ask questions about the content of a website.

Start by writing the URL of the website on the sidebar.
""")

with st.sidebar:
    url = st.text_input(
        "Write down a URL", 
        placeholder="https://example.com"
    )

if url:
    if ".xml" not in url:
        with st.sidebar:
            st.error("Please wright down a Sitemap URL.")
    else:
        # docs = load_website(url)
        retriever = load_website(url) # ê¸°ì¡´ì—ëŠ” load_websiteê°€ documentë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í–ˆì§€ë§Œ ì´ì œ retrieverë¡œ ë°˜í™˜
        docs = retriever.invoke("What is the price of GPT-4 Turbo")

        st.write(docs)